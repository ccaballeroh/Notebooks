{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mecanismos de -Atencion-.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccaballeroh/Notebooks/blob/master/Mecanismos_de_Atencion_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9MLkq8e_LXo",
        "colab_type": "text"
      },
      "source": [
        "# Auto-Atención o \"*Self-Attention*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n35f9ABT_Pu2",
        "colab_type": "text"
      },
      "source": [
        "## ¿A qué nos referimos con atención?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ZUwT1yuIQZ",
        "colab_type": "text"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/c7/63/dc/c763dc75719cd468551402142043f69b.gif\" alt=\"source: https://www.pinterest.com/pin/464574517802151786/\" width=\"35%\">\n",
        "<br>\n",
        "<a href=\"https://www.pinterest.com/pin/464574517802151786/\" target=\"_top\">Button Ball Bounce.</a>\n",
        "</center>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "La **atención** es un proceso que involucra un **enfoque selectivo** de una o pocas cosas mientras se ignoran otras. Es un **mecanismo** inspirado en la forma en que creemos funciona la vista humana.\n",
        "\n",
        "Por ejemplo, en la <u>animación anterior</u> es muy probable que una persona primero se enfoque en la bola que rebota, y luego note los botones que la bola presiona.\n",
        "\n",
        "Otro ejemplo práctico sería cuando abrimos nuestra bandeja de correo electrónico, tendemos a fijarnos únicamente en el correo no leído, ignorando los que ya hemos procesado.\n",
        "\n",
        "![picture](http://www.cs.virginia.edu/~pc9za/riiaa_2020/1.png)\n",
        "\n",
        "Una premisa de este mecanismo, es que si una persona nota que el objeto al que necesita prestar atención usualmente se ubica en una parte específica, aprenderá que es muy probable que ese objeto siga apareciendo en esa parte, y por lo tanto va a tender a enfocar su vista en dicha área.\n",
        "\n",
        "![picture](http://www.cs.virginia.edu/~pc9za/riiaa_2020/2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flwstJTeWkg4",
        "colab_type": "text"
      },
      "source": [
        "## ¿Cómo se calcula?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM2V6OkOCAhl",
        "colab_type": "text"
      },
      "source": [
        "<center>\n",
        "<img src=\"http://www.cs.virginia.edu/~pc9za/riiaa_2020/3.PNG\" alt=\"Figure 2: https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\" width=\"50%\">\n",
        "<br>\n",
        "<a href=\"https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\" target=\"_top\">Attention is all you need.</a>\n",
        "<br>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Un módulo de *self-attention* toma *n* entradas y devuelve *n* salidas.\n",
        "\n",
        "El mecanismo permite que las entradas interactúen entre sí y descubran a quién deben prestar más atención, así, relacionan diferentes posiciones de una secuencia para calcular una sola representación de esa secuencia [1].\n",
        "\n",
        "<center>\n",
        "<img src=\"https://d2l.ai/_images/attention.svg\" alt=\"Figure 10.3.2: https://d2l.ai/chapter_attention-mechanisms/transformer.html\" width=\"35%\">\n",
        "<br>\n",
        "<a href=\"https://d2l.ai/chapter_attention-mechanisms/attention.html\" target=\"_top\">Attention Mechanisms.</a>\n",
        "<br>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Podemos describir los parámetros de atención como una proyección de una consulta (**Q**uery) para una serie de pares clave-valor (**K**ey--**V**alue)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFeKqNDJzSGh",
        "colab_type": "text"
      },
      "source": [
        "Se realiza principalmente en tres pasos:\n",
        "\n",
        "*  Primero, tomamos la consulta (**Q**) y cada clave (**K**) y calculamos la similitud entre las dos para obtener los pesos.\n",
        "*  Luego usamos una función softmax para normalizar estos pesos.\n",
        "*  Finalmente ponderamos estos pesos junto con los valores correspondientes para obtener la atención final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tHWcCw0_OAB",
        "colab_type": "text"
      },
      "source": [
        "Daremos un recorrido por el algoritmo, paso por paso.\n",
        "\n",
        "### Entradas y pesos\n",
        "\n",
        "Definamos primero mi entrada, una matriz de 3x6, de manera que cada fila, represente un embedding word:\n",
        "<center>\n",
        "<img src=\"http://www.cs.virginia.edu/~pc9za/riiaa_2020/4.1.png\" alt=\"input\" width=\"25%\">\n",
        "<br>\n",
        "</center>\n",
        "\n",
        "<!-- Es decir, voy a tener 3 vectores cada uno con una dimension de 6:\n",
        "<center>\n",
        "<img src=\"http://www.cs.virginia.edu/~pc9za/riiaa_2020/4.2.png\" alt=\"input\" width=\"85%\">\n",
        "<br>\n",
        "</center> -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHIGmIXntxe0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "17a93cc1-7c83-4e58-da3a-285637cbb3ba"
      },
      "source": [
        "import torch\n",
        "\n",
        "x = [[2, 0, 1, 0, 3, 0], # e1\n",
        "     [0, 1, 5, 1, 0, 1], # e2\n",
        "     [1, 0, 1, 0, 1, 0]] # e3\n",
        "x = torch.tensor(x, dtype=torch.float32)\n",
        "print (x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 1., 0., 3., 0.],\n",
            "        [0., 2., 0., 2., 0., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjwID0FVQYRS",
        "colab_type": "text"
      },
      "source": [
        "Ahora vamos a definir un set de valores para mis pesos (**W**eights) correspondientes a mi consulta (**Q**uery), clave (**K**ey) y valor (**V**alue).\n",
        "\n",
        "<center>\n",
        "<img src=\"http://www.cs.virginia.edu/~pc9za/riiaa_2020/4.3.png\" alt=\"pesos\" width=\"60%\">\n",
        "<br>\n",
        "</center>\n",
        "\n",
        "*Estos pesos son aprendidos durante el entrenamiento de la red, aca definimos estos valores ficticios para ir paso a paso por el algoritmo*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR3YYhtBCAOd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "ebd3c2e4-59d1-484d-bbcb-aa276df6b0b1"
      },
      "source": [
        "w_query = [[1, 1, 1],\n",
        "           [0, 0, 0],\n",
        "           [0, 0, 1],\n",
        "           [0, 0, 1],\n",
        "           [1, 0, 1],\n",
        "           [0, 1, 1]]\n",
        "w_key = [[1, 1, 0],\n",
        "         [1, 0, 1],\n",
        "         [0, 1, 0],\n",
        "         [0, 0, 1],\n",
        "         [0, 0, 1],\n",
        "         [0, 1, 0]]\n",
        "w_value = [[0, 0, 5],\n",
        "           [0, 1, 0],\n",
        "           [4, 1, 0],\n",
        "           [0, 0, 5],\n",
        "           [0, 4, 0],\n",
        "           [2, 1, 0]]\n",
        "w_query = torch.tensor(w_query, dtype=torch.float32)\n",
        "w_key = torch.tensor(w_key, dtype=torch.float32)\n",
        "w_value = torch.tensor(w_value, dtype=torch.float32)\n",
        "\n",
        "print(\"W^Q:\\n{}\\n\".format(w_query))\n",
        "print(\"W^K:\\n{}\\n\".format(w_key))\n",
        "print(\"W^V:\\n{}\\n\".format(w_value))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W^Q:\n",
            "tensor([[1., 1., 1.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [0., 0., 1.],\n",
            "        [1., 0., 1.],\n",
            "        [0., 1., 1.]])\n",
            "\n",
            "W^K:\n",
            "tensor([[1., 1., 0.],\n",
            "        [1., 0., 1.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [0., 0., 1.],\n",
            "        [0., 1., 0.]])\n",
            "\n",
            "W^V:\n",
            "tensor([[0., 0., 5.],\n",
            "        [0., 1., 0.],\n",
            "        [4., 1., 0.],\n",
            "        [0., 0., 5.],\n",
            "        [0., 4., 0.],\n",
            "        [2., 1., 0.]])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ9RLPNPVsfv",
        "colab_type": "text"
      },
      "source": [
        "### Obtenemos Q, K y V\n",
        "\n",
        "Como ya tenemos los pesos, ahora podemos computar los vectores correspondientes al **Q**uery, **K**ey y **V**alue para cada vector de entrada.\n",
        "\n",
        "<center>\n",
        "<img src=\"http://www.cs.virginia.edu/~pc9za/riiaa_2020/4.4.png\" alt=\"pesos\" width=\"50%\">\n",
        "<br>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXMeGP-I1QDt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "6c490085-69ad-489e-8663-a060964a9ca0"
      },
      "source": [
        "querys = torch.matmul(x, w_query)\n",
        "keys = torch.matmul(x, w_key)\n",
        "values = torch.matmul(x, w_value)\n",
        "\n",
        "print(\"Queries:\\n{}\".format(querys))\n",
        "print(\"Keys:\\n{}\".format(keys))\n",
        "print(\"Values:\\n{}\".format(values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Queries:\n",
            "tensor([[4., 1., 5.],\n",
            "        [0., 1., 3.],\n",
            "        [2., 2., 5.]])\n",
            "Keys:\n",
            "tensor([[1., 2., 3.],\n",
            "        [2., 1., 4.],\n",
            "        [2., 3., 3.]])\n",
            "Values:\n",
            "tensor([[ 4., 13.,  5.],\n",
            "        [ 2.,  3., 10.],\n",
            "        [ 6.,  7., 10.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOpNAEY5ceKq",
        "colab_type": "text"
      },
      "source": [
        "### Puntaje de Atencion\n",
        "\n",
        "Ahora, para obtener las puntuaciones de atencion (**A**ttention **S**cores -**AS**-), vamos a calcular el producto escalar entre los **Q**ueries y los **K**eys\n",
        "\n",
        "<center>\n",
        "<img src=\"http://www.cs.virginia.edu/~pc9za/riiaa_2020/4.5.png\" alt=\"pesos\" width=\"35%\">\n",
        "<br>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU3jnFdE2SI_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "7f697eaa-5af7-404f-8441-5a849ea1f9e2"
      },
      "source": [
        "attn_scores = torch.matmul(querys, keys.t())\n",
        "print(\"Puntuaciones de Atencion:\\n{}\".format(attn_scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Puntuaciones de Atencion:\n",
            "tensor([[21., 29., 26.],\n",
            "        [11., 13., 12.],\n",
            "        [21., 26., 25.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT1vumVge7Du",
        "colab_type": "text"
      },
      "source": [
        "### Salida\n",
        "Finalmente calculamos el **softmax** de estos Attention Scores y multiplicamos por los **V**alues. \n",
        "\n",
        "<u>**El Softmax**</u> normaliza las puntuaciones, forzando que cada elemento en la fila esté dentro de un intarvalo que va de 0 a 1, y que su suma sea de 1 (distribucion de probabilidades). Esta puntuación va a ser muy alta en la posición de nuestra la palabra actual, pero además a veces es útil para interpretar a qué otra palabra el modelo presta atención, indicándonos qué es relevante para la palabra actual.\n",
        "\n",
        "<center>\n",
        "<img src=\"http://www.cs.virginia.edu/~pc9za/riiaa_2020/4.6.png\" alt=\"pesos\" width=\"35%\">\n",
        "<br>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW5rG90A3LlT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "735aa22e-a661-40be-d996-8a21be202fbb"
      },
      "source": [
        "attn_scores_softmax = torch.softmax(attn_scores / querys.shape[-1], dim=-1)\n",
        "print(\"Attention Scores despues del Softmax:\\n{}\\n\".format(attn_scores_softmax))\n",
        "\n",
        "# rendondiemos los valores para que sean mas faciles de leer...\n",
        "attn_scores_softmax = torch.round(attn_scores_softmax * 10**1) / (10**1)\n",
        "\n",
        "print(\"Attention Scores despues del Softmax (redondeados):\\n{}\".format(attn_scores_softmax))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention Scores despues del Softmax:\n",
            "tensor([[0.0483, 0.6957, 0.2559],\n",
            "        [0.2302, 0.4484, 0.3213],\n",
            "        [0.0991, 0.5248, 0.3761]])\n",
            "\n",
            "Attention Scores despues del Softmax (redondeados):\n",
            "tensor([[0.0000, 0.7000, 0.3000],\n",
            "        [0.2000, 0.4000, 0.3000],\n",
            "        [0.1000, 0.5000, 0.4000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWZbzamWm9Bh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "f17d3e3d-a2f2-43ab-a6c1-907fc008a6ea"
      },
      "source": [
        "output = torch.matmul(attn_scores_softmax, values)\n",
        "\n",
        "print(\"Salida:\\n{}\".format(output))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Salida:\n",
            "tensor([[ 3.2000,  4.2000, 10.0000],\n",
            "        [ 3.4000,  5.9000,  8.0000],\n",
            "        [ 3.8000,  5.6000,  9.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heXCefdRm8QZ",
        "colab_type": "text"
      },
      "source": [
        "O de manera alternativa, podemos computar nuestra salida de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVRKkdGC4ECA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "cda36f2e-5374-47e7-ea31-8775ab4d4558"
      },
      "source": [
        "weighted_values = attn_scores_softmax.T[:,:,None] * values[:,None]\n",
        "\n",
        "print(\"Attention Scores multiplicados por los valores:\\n{}\".format(weighted_values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention Scores multiplicados por los valores:\n",
            "tensor([[[0.0000, 0.0000, 0.0000],\n",
            "         [0.8000, 2.6000, 1.0000],\n",
            "         [0.4000, 1.3000, 0.5000]],\n",
            "\n",
            "        [[1.4000, 2.1000, 7.0000],\n",
            "         [0.8000, 1.2000, 4.0000],\n",
            "         [1.0000, 1.5000, 5.0000]],\n",
            "\n",
            "        [[1.8000, 2.1000, 3.0000],\n",
            "         [1.8000, 2.1000, 3.0000],\n",
            "         [2.4000, 2.8000, 4.0000]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A85LiZD_5rOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "d8e288bf-3e74-40e5-94f3-bc8b1568b848"
      },
      "source": [
        "outputs = weighted_values.sum(dim=0)\n",
        "\n",
        "print(\"Salida:\\n{}\".format(outputs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Salida:\n",
            "tensor([[ 3.2000,  4.2000, 10.0000],\n",
            "        [ 3.4000,  5.9000,  8.0000],\n",
            "        [ 3.8000,  5.6000,  9.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEbeTiA5l9qd",
        "colab_type": "text"
      },
      "source": [
        "De esta manera concluimos la primer seccion del taller, donde nos enfocamos en comprender cómo funcionan los mecanismos de atencion, y la matematica detras de estos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HrE4nY29p_K",
        "colab_type": "text"
      },
      "source": [
        "## Referencias\n",
        "\n",
        "[1] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. \"Attention is all you need.\" In Advances in neural information processing systems, pp. 5998-6008. 2017.\n",
        "\n"
      ]
    }
  ]
}